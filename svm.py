# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12wKbjYib-RQoA6B-Tzqw1rh8v_PQU9P6
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import files
uploaded=files.upload()
print(uploaded)

X_train=pd.read_csv("/content/train.csv")

X_train.head()

X_train.columns

l1=['PassengerId','Name','Ticket','Cabin']

X_train.drop(l1,axis=1,inplace=True)

X_train.info()

X_train.isnull().sum()

X_train["Age"].fillna(X_train["Age"].mean(),inplace=True)
X_train["Embarked"].fillna("S",inplace=True)

X_train['Embarked'].mode()

X_train.isnull().sum()

X_train.shape

X_train.head()

from sklearn.preprocessing import LabelEncoder
le_g=LabelEncoder()
le_e=LabelEncoder()

X_train["Sex"]=le_g.fit_transform(X_train["Sex"])
X_train["Embarked"]=le_e.fit_transform(X_train["Embarked"])

X_train.head()

X=X_train.iloc[:,1:].values
Y=X_train.iloc[:,0].values

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X=sc.fit_transform(X)

X[:5]

from sklearn.svm import SVC
clf1=SVC(C=100,gamma=1) # C must be high and gamma low
clf1.fit(X,Y)

pred_y1 =clf1.predict(X)

clf1.score(X,Y)

from sklearn.metrics import confusion_matrix

print(confusion_matrix(Y,pred_y1))

"""# C high → strict classification
# gamma low → smooth boundary

C  → how strict the model is
gamma → how curly the boundary is

| Feature  | Logistic    | SVM                 |
| -------- | ----------- | ------------------- |
| Output   | Probability | Class               |
| Boundary | Linear      | Linear / Non-linear |
| Robust   | Less        | More                |
| Kernel   | ❌           | ✅                   |
| Scaling  | Recommended | Mandatory           |
"""

# diagram for visualization

# Create simple 2D data
A = np.array([
    [1,2], [2,3], [3,3], [2,1],
    [6,5], [7,7], [8,6], [7,5]
])

B = np.array([0,0,0,0, 1,1,1,1])
model = SVC(kernel='linear', C=100,gamma=1)
model.fit(A,B)
def plot_svm_boundary(model, X, Y):
    plt.scatter(X[:,0], X[:,1], c=Y, cmap='coolwarm')

    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx = np.linspace(xlim[0], xlim[1], 30)
    yy = np.linspace(ylim[0], ylim[1], 30)
    YY, XX = np.meshgrid(yy, xx)
    xy = np.vstack([XX.ravel(), YY.ravel()]).T

    Z = model.decision_function(xy)
    Z = Z.reshape(XX.shape)

    # Decision boundary
    ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.7)

    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.title("SVM Decision Boundary")
    plt.show()
plot_svm_boundary(model, A,B)

"""SVM decision boundary visualization requires reducing features to 2D; the plotted boundary is an approximation for intuition, not the full model.

❌ This diagram is NOT your real model
✅ It is only for understanding

Your real model:

SVC(C=100, gamma=1)


works in multi-dimensional space, which humans cannot see

# ONE MORE EXAMPLE
"""

# More complex non-linear data
C = np.array([
    [1,2], [2,3], [3,2], [2,1],
    [4,4], [5,3], [3,5], [4,2],
    [6,6], [7,5], [5,7], [6,4]
])

# Mixed labels (non-linearly separable)
D = np.array([0,0,1,1, 0,1,0,1, 1,0,1,0])
model = SVC(kernel='rbf', C=10, gamma=1)
model.fit(C, D)
plot_svm_boundary(model, C, D)

for g in [0.1, 1, 10]:
    model = SVC(kernel='rbf', C=10, gamma=g)
    model.fit(C, D)
    plot_svm_boundary(model, A, B)
    #gamma = 0.1 → smooth boundary
    # gamma = 10 → very wiggly (overfitting)

  #“Curved decision boundaries in SVM are achieved using non-linear kernels like RBF, which implicitly map data into higher-dimensional space.”

"""| Parameter     | Effect on Boundary |
| ------------- | ------------------ |
| kernel=linear | Straight line      |
| kernel=rbf    | Curved line        |
| Small C       | Wide margin        |
| Large C       | Narrow margin      |
| Small gamma   | Smooth boundary    |
| Large gamma   | Complex boundary   |

"""